# プログラムの内容
破滅的忘却を軽減するニューラルネットワークを用いたスパムフィルタ

## 破滅的忘却
近年、ニューラルネットワークを用いた文章分類が活発に行われており、スパムフィルタへの応用が期待されています。 

しかし、学習済みのニューラルネットワークに、新たなデータBを追加学習させると、その新たなデータに対する正解率は上がるが、
以前に学習したデータAに対する正解率は低下してしまいます。 

これは、ニューラルネットワークが以前のデータAを学習したときに得られたパラメータ(重みやバイアス)の値は、
そのデータAの正解率を上げることに適したパラメータになっているため、そこの新たなデータBを追加学習させると、
ニューラルネットワークのパラメータは新たなデータBの正解率を上げるためのパラメータへと更新されてしまうため、
以前に学習したデータAに対しての正解率が低下してしまうためです。 

このように、ニューラルネットワークに新たなデータを追加学習させると、以前のデータを学習して得られたパラメータを忘却してしまう(新たなデータに適したパラメータに更新される)ことを**破滅的忘却**といいます。

# 既存手法
ニューラルネットワークの課題の一つである**破滅的忘却**を解決する既存手法としては、学習させたい新しいデータがある場合、過去のデータを学習したときに得られたニューラルネットワークのパラメータは捨てて、過去のデータとその新しいデータをまとめて新たにニューラルネットワークに学習させることで、過去のデータと新しいデータの両方のデータに対する正解率を算出するという手法をとっていました。

しかし、この手法では以下の問題点が挙げられます。
- 過去に学習したデータを保持し続ける必要がある
- 学習時間が増加する

こういった問題点があるなか、近年、過去に学習したデータを保持せずに新たなデータを追加学習させても、破滅的忘却を軽減できる**Elastic Weight Consolidation(EWC)**と呼ばれる手法が注目を集めています。

## Elastic Weight Consolidation(EWC)
Elastic Weight Consolidationでは、過去に学習したデータの正解率を保持するために重要なパラメータの要素(例えば重みの何行何列目の要素など)を確率的に導き出し、新たなデータを学習するときに、そのパラメータの要素の更新量を抑えことで、過去のデータに対する正解率をなるべく保持しつつ、新たなデータに対する正解率も上げることができる。

ニューラルネットワークの出力値と正解データの誤差を計算し，その誤差が最小になるように重みやバイアスの更新を行う．
誤差の計算方法として，テキスト分類等のクラス分類では交差エントロピーと呼ばれる損失関数が一般的に使用され，以下の式で計算される．
$$ \begin{eqnarray} L=-\sum_{i=1}^{n}t_i\logy_i\end{eqnarray} $$


# 論文 URL
[論文説明](https://www.ieice.org/ken/paper/20190723N1Of/)

# 学会　受賞歴
[SITE学術奨励賞　2019年7月　「破滅的忘却を軽減するニューラルネットワークを用いたスパムフィルタの提案」](https://www.ieice.org/~site/site_award.html)